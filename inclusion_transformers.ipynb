{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy as sp\n",
    "import shap\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_curve, PrecisionRecallDisplay, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import trange\n",
    "from ray import tune\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR PATH HERE\n",
    "filepath = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in all df: 3933, Number of samples in test df: 800, Number of samples in train_val df: 3133\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(filepath + '/data/second_gen/ratings.csv')[1:].fillna('')\n",
    "df['labels'] = df.apply(lambda x: x['rating_consensus'] if x['rating_consensus'] != '' else x['rating1'] if ((x['rating1'] == x['rating2']) and (x['consensus_reason'] == '') and (x['rating1'] != '')) else 'error', axis=1)\n",
    "test_df = df[-800:]\n",
    "train_val_df = df[:-800]\n",
    "print('Number of samples in all df: {}, Number of samples in test df: {}, Number of samples in train_val df: {}'.format(len(df),len(test_df), len(train_val_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 2506\n",
      "Length of val set: 627\n",
      "Number of included in train set: 171.0 for ratio: 0.068\n",
      "Number of included in val set: 50.0 for ratio: 0.080\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split (train_val_df, test_size=0.2)\n",
    "\n",
    "print('Length of train set: {}'.format(len(train_df)))\n",
    "print('Length of val set: {}'.format(len(val_df)))\n",
    "print('Number of included in train set: {} for ratio: {:.3f}'.format(train_df['labels'].sum(), train_df['labels'].sum()/len(train_df)))\n",
    "print('Number of included in val set: {} for ratio: {:.3f}'.format(val_df['labels'].sum(), val_df['labels'].sum()/len(val_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_to_keep = ['text', 'labels']\n",
    "train_val_df = train_val_df[df_columns_to_keep]\n",
    "train_df = train_df[df_columns_to_keep]\n",
    "val_df = val_df[df_columns_to_keep]\n",
    "test_df = test_df[df_columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_ds = Dataset.from_pandas(train_val_df)\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds['train_val'] = train_val_ds\n",
    "ds['train'] = train_ds\n",
    "ds['val'] = val_ds\n",
    "ds['test'] = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stringifying the column: 100%|██████████| 4/4 [00:00<00:00, 999.54ba/s]\n",
      "Casting to class labels: 100%|██████████| 4/4 [00:00<00:00, 999.00ba/s]\n",
      "Casting the dataset: 100%|██████████| 1/1 [00:00<00:00, 83.26ba/s]\n",
      "Stringifying the column: 100%|██████████| 3/3 [00:00<00:00, 999.28ba/s]\n",
      "Casting to class labels: 100%|██████████| 3/3 [00:00<00:00, 998.88ba/s]\n",
      "Casting the dataset: 100%|██████████| 1/1 [00:00<00:00, 142.75ba/s]\n",
      "Stringifying the column: 100%|██████████| 1/1 [00:00<00:00, 998.64ba/s]\n",
      "Casting to class labels: 100%|██████████| 1/1 [00:00<00:00, 999.60ba/s]\n",
      "Casting the dataset: 100%|██████████| 1/1 [00:00<00:00, 499.86ba/s]\n",
      "Stringifying the column: 100%|██████████| 1/1 [00:00<00:00, 999.12ba/s]\n",
      "Casting to class labels: 100%|██████████| 1/1 [00:00<00:00, 999.12ba/s]\n",
      "Casting the dataset: 100%|██████████| 1/1 [00:00<00:00, 999.83ba/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.class_encode_column('labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  6.98ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  6.67ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.73ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.75ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_function(samples):\n",
    "    return tokenizer(samples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = ds.map(tokenizer_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_label_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metrics = {'accuracy':accuracy_score(labels, predictions), 'f1':f1_score(labels, predictions), 'precision':precision_score(labels, predictions), 'recall':recall_score(labels, predictions)}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', num_labels=2)\n",
    "\n",
    "hp_space = {\n",
    "    'learning_rate':tune.choice([5e-5, 2e-5, 1e-5, 8e-6, 5e-6, 2e-6, 1e-6]),\n",
    "    'num_train_epochs':tune.choice([1,2,3,4,5,6]),\n",
    "    'per_device_train_batch_size':8\n",
    "}\n",
    "training_args = TrainingArguments(filepath + '/results/inclusion/run1', evaluation_strategy='epoch', save_strategy='no', disable_tqdm=True)\n",
    "trainer = Trainer(model_init=model_init, args=training_args, compute_metrics=compute_single_label_metrics, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['val'])\n",
    "best_model = trainer.hyperparameter_search(hp_space=lambda _: hp_space, direction='maximize', n_trials=40, local_dir=filepath + '/results/inclusion/run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', num_labels=2)\n",
    "\n",
    "hp_space = {\n",
    "    'learning_rate':tune.choice([3e-5, 2e-5, 1.2e-5, 1e-5, 8e-6]),\n",
    "    'num_train_epochs':tune.choice([1,2,3,4,6,8,10]),\n",
    "    'per_device_train_batch_size':8\n",
    "}\n",
    "training_args = TrainingArguments(filepath + '/results/inclusion/run2', evaluation_strategy='epoch', save_strategy='no', disable_tqdm=True)\n",
    "trainer = Trainer(model_init=model_init, args=training_args, compute_metrics=compute_single_label_metrics, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['val'])\n",
    "best_model = trainer.hyperparameter_search(hp_space=lambda _: hp_space, direction='maximize', n_trials=40, local_dir=filepath + '/results/inclusion/transformers_hp_search/ray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', num_labels=2)\n",
    "\n",
    "hp_space = {\n",
    "    'learning_rate':tune.choice([3e-5, 2e-5, 1.2e-5, 1e-5, 8e-6]),\n",
    "    'num_train_epochs':tune.choice([1,2,3,4,6,8,10]),\n",
    "    'per_device_train_batch_size':8\n",
    "}\n",
    "training_args = TrainingArguments(filepath + '/results/inclusion/run2', evaluation_strategy='epoch', save_strategy='no', disable_tqdm=True)\n",
    "trainer = Trainer(model_init=model_init, args=training_args, compute_metrics=compute_single_label_metrics, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['val'])\n",
    "best_model = trainer.hyperparameter_search(hp_space=lambda _: hp_space, direction='maximize', n_trials=40, local_dir=filepath + '/results/inclusion/run2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', num_labels=2)\n",
    "\n",
    "hp_space = {\n",
    "    'learning_rate':tune.choice([1.5e-5, 1.2e-5, 1e-5]),\n",
    "    'num_train_epochs':tune.choice([10]),\n",
    "    'per_device_train_batch_size':8\n",
    "}\n",
    "training_args = TrainingArguments(filepath + '/results/inclusion/run3', evaluation_strategy='epoch', save_strategy='no', disable_tqdm=True)\n",
    "trainer = Trainer(model_init=model_init, args=training_args, compute_metrics=compute_single_label_metrics, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['val'])\n",
    "best_model = trainer.hyperparameter_search(hp_space=lambda _: hp_space, direction='maximize', n_trials=40, local_dir=filepath + '/results/inclusion/run3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 3133\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1568\n",
      " 32%|███▏      | 500/1568 [03:19<07:05,  2.51it/s]Saving model checkpoint to ./models/test_set_eval_models/inclusion_biobert\\checkpoint-500\n",
      "Configuration saved in ./models/test_set_eval_models/inclusion_biobert\\checkpoint-500\\config.json\n",
      "Model weights saved in ./models/test_set_eval_models/inclusion_biobert\\checkpoint-500\\pytorch_model.bin\n",
      " 64%|██████▍   | 1000/1568 [06:40<03:44,  2.53it/s]Saving model checkpoint to ./models/test_set_eval_models/inclusion_biobert\\checkpoint-1000\n",
      "Configuration saved in ./models/test_set_eval_models/inclusion_biobert\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models/test_set_eval_models/inclusion_biobert\\checkpoint-1000\\pytorch_model.bin\n",
      " 96%|█████████▌| 1500/1568 [10:02<00:27,  2.48it/s]Saving model checkpoint to ./models/test_set_eval_models/inclusion_biobert\\checkpoint-1500\n",
      "Configuration saved in ./models/test_set_eval_models/inclusion_biobert\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./models/test_set_eval_models/inclusion_biobert\\checkpoint-1500\\pytorch_model.bin\n",
      "100%|██████████| 1568/1568 [10:32<00:00,  2.80it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1568/1568 [10:32<00:00,  2.48it/s]\n",
      "Saving model checkpoint to ./models/test_set_eval_models/inclusion_biobert/model\n",
      "Configuration saved in ./models/test_set_eval_models/inclusion_biobert/model\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 633.0953, 'train_samples_per_second': 19.795, 'train_steps_per_second': 2.477, 'train_loss': 0.11762488618188975, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./models/test_set_eval_models/inclusion_biobert/model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', num_labels=2)\n",
    "training_args = TrainingArguments(filepath + '/models/test_set_eval_models/inclusion_biobert', evaluation_strategy='no', logging_strategy='no', learning_rate=1e-5, num_train_epochs=4)\n",
    "trainer = Trainer(model=model, args=training_args, compute_metrics=compute_single_label_metrics, train_dataset=tokenized_datasets['train_val'])\n",
    "trainer.train()\n",
    "trainer.save_model(filepath + '/models/test_set_eval_models/inclusion_biobert/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 800\n",
      "  Batch size = 8\n",
      "100%|██████████| 100/100 [00:12<00:00,  7.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16631275415420532,\n",
       " 'eval_accuracy': 0.95875,\n",
       " 'eval_f1': 0.717948717948718,\n",
       " 'eval_precision': 0.6666666666666666,\n",
       " 'eval_recall': 0.7777777777777778,\n",
       " 'eval_runtime': 12.8159,\n",
       " 'eval_samples_per_second': 62.422,\n",
       " 'eval_steps_per_second': 7.803,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine thresholds to automatically include or exclude papers\n",
    "Recall = 1 or precision = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/config.json from cache at C:\\Users\\maxim/.cache\\huggingface\\transformers\\ece5e89bab3b63a40e413c7f599e6081663cad06eb394e48d5023930733d15a3.ad895c9bc4687ffedea1a4cc498ac3f67ebd2083732981c2a06f548cde7d6582\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/vocab.txt from cache at C:\\Users\\maxim/.cache\\huggingface\\transformers\\e1dbe55c1d83f79c61148b28fbe20bfbb6aeb2f7163225830b3063fda58425e5.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/config.json from cache at C:\\Users\\maxim/.cache\\huggingface\\transformers\\ece5e89bab3b63a40e413c7f599e6081663cad06eb394e48d5023930733d15a3.ad895c9bc4687ffedea1a4cc498ac3f67ebd2083732981c2a06f548cde7d6582\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/config.json from cache at C:\\Users\\maxim/.cache\\huggingface\\transformers\\ece5e89bab3b63a40e413c7f599e6081663cad06eb394e48d5023930733d15a3.ad895c9bc4687ffedea1a4cc498ac3f67ebd2083732981c2a06f548cde7d6582\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file ./models/test_set_eval_models/inclusion_biobert/model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./models/test_set_eval_models/inclusion_biobert/model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./models/test_set_eval_models/inclusion_biobert/model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(filepath + '/models/test_set_eval_models/inclusion_biobert/model/', local_files_only=True)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff29eb727a2487f9496f592076d316d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in trange(len(test_ds)):\n",
    "    scores.append(pipe(test_ds[i]['text'], **tokenizer_kwargs)[0][1]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x200fe9293d0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PrecisionRecallDisplay.from_predictions(test_ds['labels'], scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr,re,th = precision_recall_curve(test_ds['labels'], scores)\n",
    "rth_df = pd.DataFrame.from_dict({'precision':pr[:-1],'recall':re[:-1],'threshold':th})\n",
    "pth_df = rth_df.reindex(index=rth_df.index[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.355263\n",
       "recall       1.000000\n",
       "threshold    0.000826\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceiling_recall = 0.99\n",
    "rth_df[rth_df.recall >= ceiling_recall].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts excluded from review based on recall threshold: 648 out of 800 for ratio of 0.810\n"
     ]
    }
   ],
   "source": [
    "recall_threshold = 0.000826\n",
    "recall_test = []\n",
    "for i, label, score in zip(range(len(test_ds['labels'])), test_ds['labels'], scores):\n",
    "    if score >= recall_threshold:\n",
    "        recall_test.append({'index':i, 'label':label, 'score':score})\n",
    "print('Number of abstracts excluded from review based on recall threshold: {} out of {} for ratio of {:.3f}'.format(len(test_ds) - len(recall_test), len(test_ds), (len(test_ds) - len(recall_test)) / len(test_ds)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1.0, Score: 0.9916127920150757\n",
      "Pharmacist-Driven Methicillin-Resistant S. aureus Polymerase Chain Reaction Testing for Pneumonia. BACKGROUND Nasal colonization with methicillin-resistant Staphylococcus aureus (MRSA) can be detected using nasal swab polymerase chain reaction (PCR) assay and is associated with clinical MRSA infection. The MRSA nasal PCR has a rapid turnaround time and a negative predictive value for MRSA pneumonia of >98%; however, data are limited in critically ill patients. OBJECTIVE The purpose of this study is to determine the impact of a pharmacist-driven algorithm, utilizing MRSA PCR nasal screening on duration of anti-MRSA therapy in patients admitted to the intensive care unit (ICU) with suspected pneumonia. METHODS A single-center pre/post study was conducted in 4 ICUs at a large tertiary care community hospital. Adult patients admitted to the ICU initiated on vancomycin or linezolid for pneumonia managed using a pharmacist-driven MRSA PCR algorithm were included in the algorithm cohort. A historical cohort with standard management was matched 1:1 by age, type of pneumonia, and Acute Physiology and Chronic Health Evaluation II (APACHE II) score. The primary outcome was duration of anti-MRSA therapy. Secondary outcomes included MRSA rates, number of vancomycin levels, new onset of acute kidney injury (AKI), ICU length of stay (LOS), hospital LOS, and mortality. RESULTS Of the 245 patients screened, 50 patients met inclusion criteria for the algorithm cohort and were matched to 50 patients in the historical cohort. The duration of anti-MRSA therapy was significantly lower compared with the historical cohort (47 vs 95 hours; P < 0.001). Secondary outcomes were similar between groups for MRSA rates, new onset of AKI, LOS, and mortality. There were less vancomycin levels ordered in the algorithm cohort (2 vs 3, P = 0.026). CONCLUSIONS A pharmacist-driven MRSA PCR algorithm significantly reduced anti-MRSA duration of therapy in critically ill patients with pneumonia. Future studies should validate these results in critically ill populations and in settings where MRSA pneumonia is more prevalent.\n"
     ]
    }
   ],
   "source": [
    "i = 28\n",
    "print('Label: {}, Score: {}\\n{}'.format(recall_test[i]['label'], recall_test[i]['score'], test_ds[recall_test[i]['index']]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    1.000000\n",
       "recall       0.037037\n",
       "threshold    0.995476\n",
       "Name: 150, dtype: float64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceiling_precision = 0.88\n",
    "pth_df[pth_df.precision >= ceiling_precision].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts excluded from review based on precision threshold: 1 out of 800 for ratio of 0.001\n"
     ]
    }
   ],
   "source": [
    "precision_threshold = 0.995476\n",
    "precision_test = []\n",
    "for i, label, score in zip(range(len(test_ds['labels'])), test_ds['labels'], scores):\n",
    "    if score >= precision_threshold:\n",
    "        precision_test.append({'index':i, 'label':label, 'score':score})\n",
    "print('Number of abstracts excluded from review based on precision threshold: {} out of {} for ratio of {:.3f}'.format(len(precision_test), len(test_ds), len(precision_test) / len(test_ds)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1.0, Score: 0.9959228038787842\n",
      "Implementation of a pharmacist-led diabetes management service in an endocrinology clinic. BACKGROUND Diabetes is a complicated health condition that can lead to significant health complications. Pharmacists are in an ideal position to make therapeutic interventions, provide clinical education, and can provide necessary follow-up to evaluate response to therapy for patients with diabetes. OBJECTIVES The primary objective of this study is to evaluate the mean change in hemoglobin A1c (HbA1c) in patients receiving short-term diabetes management services from a clinical pharmacist through collaborative drug therapy management. METHODS This study is a single-center retrospective chart review of patients with diabetes who have been referred by their endocrinologist to the clinical pharmacist for short-term intensification of pharmacologic management of hyperglycemia. Patients included in the study completed at least 2 visits with the pharmacist during the study period. The primary outcome was to evaluate the mean absolute change in HbA1c at 3-6 months from baseline. RESULTS Data were collected from 117 patients. The average age was 55 years (19-91 years, SD ± 14.5), 65 patients (55.6%) were female, average duration of diabetes was 14.9 years (0.5-49 years, SD ± 9.9), 21 patients (17.9%) had type 1 diabetes, 96 patients (82.1%) had type 2 diabetes, and 88 patients (75.2%) had a baseline HbA1c of at least 8.5%. On average, patients reduced their HbA1c by 2.0% (P < 0.001) at 3-6 months. For patients with a baseline HbA1c of at least 8.5%, they experienced a 2.5% (P < 0.001) reduction in HbA1c at 3-6 months. CONCLUSION The addition of a clinical pharmacist within the endocrinology practice was associated with significant improvements in glycemic control for those referred. This short-term, intensive service model demonstrates that patients can achieve significant reductions in HbA1c with temporary support from a clinical pharmacist.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print('Label: {}, Score: {}\\n{}'.format(precision_test[i]['label'], precision_test[i]['score'], test_ds[precision_test[i]['index']]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stringifying the column: 100%|██████████| 4/4 [00:00<00:00, 666.11ba/s]\n",
      "Casting to class labels: 100%|██████████| 4/4 [00:00<00:00, 666.05ba/s]\n",
      "Casting the dataset: 100%|██████████| 1/1 [00:00<00:00, 62.44ba/s]\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/config.json from cache at C:\\Users\\maxim/.cache\\huggingface\\transformers\\ece5e89bab3b63a40e413c7f599e6081663cad06eb394e48d5023930733d15a3.ad895c9bc4687ffedea1a4cc498ac3f67ebd2083732981c2a06f548cde7d6582\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/vocab.txt from cache at C:\\Users\\maxim/.cache\\huggingface\\transformers\\e1dbe55c1d83f79c61148b28fbe20bfbb6aeb2f7163225830b3063fda58425e5.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/config.json from cache at C:\\Users\\maxim/.cache\\huggingface\\transformers\\ece5e89bab3b63a40e413c7f599e6081663cad06eb394e48d5023930733d15a3.ad895c9bc4687ffedea1a4cc498ac3f67ebd2083732981c2a06f548cde7d6582\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/config.json from cache at C:\\Users\\maxim/.cache\\huggingface\\transformers\\ece5e89bab3b63a40e413c7f599e6081663cad06eb394e48d5023930733d15a3.ad895c9bc4687ffedea1a4cc498ac3f67ebd2083732981c2a06f548cde7d6582\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.47ba/s]\n"
     ]
    }
   ],
   "source": [
    "all_df = df[df_columns_to_keep]\n",
    "all_ds = Dataset.from_pandas(all_df)\n",
    "all_ds = all_ds.class_encode_column('labels')\n",
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "\n",
    "def tokenizer_function(samples):\n",
    "    return tokenizer(samples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_all_ds = all_ds.map(tokenizer_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/config.json from cache at C:\\Users\\maxim/.cache\\huggingface\\transformers\\ece5e89bab3b63a40e413c7f599e6081663cad06eb394e48d5023930733d15a3.ad895c9bc4687ffedea1a4cc498ac3f67ebd2083732981c2a06f548cde7d6582\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/pytorch_model.bin from cache at C:\\Users\\maxim/.cache\\huggingface\\transformers\\1414d44e1692194ad8c160a4bc51d9bd98da5365a8c08e5182731cf84eb134f5.02df0b4a307fbaed7d1fe54d7b95b850ac1f31e1331142c3da71b82b0d48f77d\n",
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 3133\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1568\n",
      " 32%|███▏      | 500/1568 [03:18<07:04,  2.52it/s]Saving model checkpoint to ./models/production_models/inclusion_biobert/v0.1\\checkpoint-500\n",
      "Configuration saved in ./models/production_models/inclusion_biobert/v0.1\\checkpoint-500\\config.json\n",
      "Model weights saved in ./models/production_models/inclusion_biobert/v0.1\\checkpoint-500\\pytorch_model.bin\n",
      " 64%|██████▍   | 1000/1568 [06:39<03:45,  2.52it/s]Saving model checkpoint to ./models/production_models/inclusion_biobert/v0.1\\checkpoint-1000\n",
      "Configuration saved in ./models/production_models/inclusion_biobert/v0.1\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./models/production_models/inclusion_biobert/v0.1\\checkpoint-1000\\pytorch_model.bin\n",
      " 96%|█████████▌| 1500/1568 [09:59<00:26,  2.53it/s]Saving model checkpoint to ./models/production_models/inclusion_biobert/v0.1\\checkpoint-1500\n",
      "Configuration saved in ./models/production_models/inclusion_biobert/v0.1\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./models/production_models/inclusion_biobert/v0.1\\checkpoint-1500\\pytorch_model.bin\n",
      "100%|██████████| 1568/1568 [10:29<00:00,  2.79it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1568/1568 [10:29<00:00,  2.49it/s]\n",
      "Saving model checkpoint to ./models/production_models/inclusion_biobert/v0.1\n",
      "Configuration saved in ./models/production_models/inclusion_biobert/v0.1\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 629.5655, 'train_samples_per_second': 19.906, 'train_steps_per_second': 2.491, 'train_loss': 0.11029928557726802, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./models/production_models/inclusion_biobert/v0.1\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model_version = '0.1'\n",
    "model = AutoModelForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.2', num_labels=2)\n",
    "training_args = TrainingArguments(filepath + '/models/production_models/inclusion_biobert/v{}'.format(model_version), evaluation_strategy='no', logging_strategy='no', learning_rate=1e-5, num_train_epochs=4)\n",
    "trainer = Trainer(model=model, args=training_args, compute_metrics=compute_single_label_metrics, train_dataset=tokenized_datasets['train_val'])\n",
    "trainer.train()\n",
    "trainer.save_model(filepath + '/models/production_models/inclusion_biobert/v{}'.format(model_version))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('impact')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b865e41611128619125efb3de810fd1945d838f9916dd1a16e8a8b77ba5194d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
